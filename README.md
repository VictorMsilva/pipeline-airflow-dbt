# üèóÔ∏è Pipeline de Gera√ß√£o e Transforma√ß√£o de Dados Sint√©ticos com Docker, Airflow + DBT

## üìå Resumo do Projeto
Pipeline automatizado e modular para gerar, transformar e disponibilizar dados sint√©ticos de uma empresa fict√≠cia de materiais de limpeza, utilizando containers Docker para facilitar a execu√ß√£o e reprodutibilidade do ambiente.

---

## ü©∫ Contexto e Problema
Empresas de varejo e atacado precisam analisar dados de vendas, clientes e estoque, mas nem sempre possuem bases de dados estruturadas para testes. A constru√ß√£o manual desses dados pode ser trabalhosa e pouco escal√°vel.

Este projeto resolve o problema criando um pipeline baseado em Docker, que gera, transforma e disponibiliza tabelas anal√≠ticas para simular um ambiente de vendas e log√≠stica.

---

## üéØ Objetivos
- Gerar dados sint√©ticos de vendas, clientes e estoque
- Containerizar o ambiente para facilitar deploy e reprodu√ß√£o
- Automatizar a transforma√ß√£o e valida√ß√£o dos dados
- Construir um pipeline escal√°vel com Airflow e DBT
- Simular relat√≥rios de BI para an√°lise de desempenho

---

## üß∞ Stack Tecnol√≥gica
- **Containers:** Docker + Docker Compose
- **Orquestra√ß√£o:** Apache Airflow
- **Transforma√ß√£o:** DBT (Data Build Tool)
- **Armazenamento:** PostgreSQL (em container)
- **Gera√ß√£o de Dados:** Faker, pandas
- **Visualiza√ß√£o (opcional):** Metabase, Power BI ou Streamlit

---

## ‚öôÔ∏è Estrutura do Projeto

```plaintext
pipeline-airflow-dbt/
‚îú‚îÄ‚îÄ dags/                        # DAGs do Airflow (orquestra√ß√£o)
‚îÇ   ‚îî‚îÄ‚îÄ example_dag.py
‚îú‚îÄ‚îÄ dbt/                         # Projeto DBT (transforma√ß√µes anal√≠ticas)
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ example_model.sql
‚îÇ   ‚îú‚îÄ‚îÄ seeds/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ example_seed.csv
‚îÇ   ‚îú‚îÄ‚îÄ dbt_project.yml
‚îÇ   ‚îî‚îÄ‚îÄ profiles.yml
‚îú‚îÄ‚îÄ data/                        # Dados em diferentes est√°gios
‚îÇ   ‚îú‚îÄ‚îÄ raw/
‚îÇ   ‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îî‚îÄ‚îÄ analytics/
‚îú‚îÄ‚îÄ src/                         # C√≥digo-fonte principal
‚îÇ   ‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ generate_data.py
‚îÇ   ‚îú‚îÄ‚îÄ processing/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cleaning.py
‚îÇ   ‚îú‚îÄ‚îÄ transformation/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ enrich.py
‚îÇ   ‚îú‚îÄ‚îÄ validation/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schema_check.py
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ logger.py
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ scripts/                     # Scripts auxiliares
‚îÇ   ‚îú‚îÄ‚îÄ run_pipeline.py
‚îÇ   ‚îî‚îÄ‚îÄ setup_env.py
‚îú‚îÄ‚îÄ tests/                       # Testes unit√°rios e integra√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ ingestion/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_generate_data.py
‚îÇ   ‚îú‚îÄ‚îÄ processing/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_cleaning.py
‚îÇ   ‚îú‚îÄ‚îÄ transformation/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_enrich.py
‚îÇ   ‚îú‚îÄ‚îÄ validation/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_schema_check.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ test_logger.py
‚îú‚îÄ‚îÄ notebooks/                   # Notebooks para explora√ß√£o
‚îÇ   ‚îî‚îÄ‚îÄ exploracao_dados.ipynb
‚îú‚îÄ‚îÄ configs/                     # Configura√ß√µes e vari√°veis
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml
‚îÇ   ‚îî‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ docker-compose.yml
‚îî‚îÄ‚îÄ README.md
```

---

## üì¶ Modelagem Dimensional ‚Äì Projeto Vendas

### üß† Processo de Neg√≥cio
Venda de produtos por clientes, registrados em pedidos e entregues em endere√ßos, com pagamentos e avalia√ß√µes.

### üìè Gr√£o da Fato Principal
Cada linha representa um item vendido em um pedido.

### üìä Tabela Fato: `fato_vendas`
- id_item_pedido
- id_pedido
- id_cliente
- id_produto
- id_vendedor
- id_data_pedido
- quantidade
- preco_unitario
- valor_total

### üß© Tabelas de Dimens√£o

**dim_cliente**
- id_cliente
- nome
- tipo_cliente
- telefone
- email
- cidade
- estado
- data_cadastro

**dim_produto**
- id_produto
- nome_produto
- descricao
- preco_unitario
- nome_categoria
- nome_fornecedor
- cidade_fornecedor
- estado_fornecedor

**dim_vendedor**
- id_vendedor
- nome
- telefone
- email
- nome_departamento
- data_contratacao

**dim_data**
- id_data
- data
- dia
- m√™s
- ano
- trimestre
- dia_da_semana
- feriado

---

## ‚≠ê Star Schema

Representa√ß√£o visual da modelagem dimensional proposta:

![Esquema Estrela ‚Äì Modelagem Dimensional](docs/Star_schema_vendas.png)

---

## üèóÔ∏è Camadas do Projeto

### 1. Camada RAW / Staging (`models/staging/`)
Tratamento dos dados brutos:
- Limpeza de inconsist√™ncias
- Padroniza√ß√£o de formatos
- Elimina√ß√£o de nulos e duplicatas
- Convers√µes de tipos (ex: datas, num√©ricos)

**Modelos inclu√≠dos:**
- `stg_clientes.sql`, `stg_enderecos.sql`, `stg_produtos.sql`
- `stg_vendedores.sql`, `stg_departamentos.sql`, `stg_pedidos.sql`
- `stg_itens_pedido.sql`, `stg_pagamentos.sql`, `stg_entregas.sql`
- `stg_categorias.sql`, `stg_fornecedores.sql`, `stg_datas.sql`, `stg_feriados.sql`

### 2. Camada de Modelagem (`models/marts/`)
Modelos incrementais que inserem dados diretamente nas tabelas f√≠sicas criadas no banco. Cada dimens√£o √© populada e atualizada de forma incremental.

**Modelos inclu√≠dos:**
- `dim_cliente.sql`
- `dim_produto.sql`
- `dim_vendedor.sql`
- `dim_pagamento.sql`
- `dim_entrega.sql`
- `dim_data.sql`
- `fato_vendas.sql`

## üß™ Testes Automatizados

Cada dimens√£o e fato tem testes definidos via arquivos `.yml`, como:
- `not_null` ‚Äî campos obrigat√≥rios
- `unique` ‚Äî chaves prim√°rias
- `accepted_values` ‚Äî dom√≠nios v√°lidos
- `relationships` ‚Äî integridade referencial

## üìö Documenta√ß√£o DBT e Data Lineage

O projeto utiliza o [DBT Docs](https://docs.getdbt.com/docs/building-a-dbt-project/documentation) para documentar todos os modelos, fontes, testes e depend√™ncias do pipeline de dados.

- Para visualizar a documenta√ß√£o interativa e o lineage graph, rode:

  dbt docs generate
  dbt docs serve --port 8080

## üö¶ Orquestra√ß√£o com Apache Airflow

O projeto utiliza o [Apache Airflow](https://airflow.apache.org/) para orquestrar e automatizar o pipeline de dados, garantindo que as etapas de ingest√£o, transforma√ß√£o (DBT) e valida√ß√£o sejam executadas de forma controlada e agendada.

- As DAGs (pipelines) ficam na pasta `dags/`.
- O Airflow √© executado em container Docker, acess√≠vel em [http://localhost:8080](http://localhost:8080).

## ‚úÖ Boas Pr√°ticas de Engenharia de Dados

- **Separa√ß√£o de camadas:** Dados brutos, staging e modelos anal√≠ticos organizados em pastas distintas.
- **Versionamento e reprodutibilidade:** Todo o pipeline √© versionado no Git e execut√°vel via Docker.
- **Gerenciamento seguro de vari√°veis:** Credenciais e par√¢metros sens√≠veis em arquivos de ambiente, nunca no c√≥digo.
- **Testes automatizados:** DBT garante integridade dos dados com testes de unicidade, nulidade e relacionamentos.
- **Orquestra√ß√£o modular:** Airflow agenda, monitora e facilita a extens√£o dos pipelines.
- **Documenta√ß√£o e lineage:** DBT Docs gera documenta√ß√£o autom√°tica e visualiza√ß√£o do fluxo de dados.

## üìà Entrega de Resultados

- **Tabelas anal√≠ticas validadas:** Prontas para BI e an√°lises.
- **Pipeline automatizado:** Da ingest√£o √† disponibiliza√ß√£o dos dados, tudo monitorado pelo Airflow.
- **Ambiente reproduz√≠vel:** Qualquer pessoa pode clonar e rodar o projeto do zero.
- **Pronto para integra√ß√£o:** Dados finais conect√°veis a ferramentas como Metabase ou Power BI.

---

## üöÄ Como configurar o ambiente Python (recomendado: Python 3.11)

> **Aten√ß√£o:** O Apache Airflow 2.8.x n√£o √© compat√≠vel com Python 3.12. Use Python 3.11 para evitar erros de instala√ß√£o.

### 1. Instale o Python 3.11 (no Ubuntu/WSL)
```bash
sudo apt update
sudo apt install python3.11 python3.11-venv python3.11-distutils
```

### 2. Crie e ative o ambiente virtual
```bash
python3.11 -m venv .venv
source .venv/bin/activate
```

### 3. Instale as depend√™ncias do projeto
```bash
pip install --upgrade pip
pip install -r requirements.txt
pip install apache-airflow==2.8.4
```

---

## üöÄ Como executar o projeto com Docker

### 1. Instale o Docker e o Docker Compose (caso ainda n√£o tenha)

No Ubuntu/WSL:
```bash
sudo apt update
sudo apt install -y docker.io docker-compose
```


### 2. Suba os containers do projeto

Na raiz do projeto, execute:
```bash
docker-compose up -d
```
Isso ir√° iniciar todos os servi√ßos necess√°rios (Airflow, DBT, banco de dados, etc) em containers.

### 3. Gere e carregue os dados sint√©ticos no banco

Execute o script SQL de gera√ß√£o de dados sint√©ticos dentro do container do banco de dados:
```bash
docker-compose exec db psql -U admin -d empresa -f /scripts/SQL/start_gen_dados_sinteticos.sql
```

### 4. Execute o pipeline DBT

Dentro do container DBT, rode:
```bash
docker-compose exec dbt dbt run
docker-compose exec dbt dbt test
```
Esses comandos v√£o criar os modelos anal√≠ticos e rodar os testes de qualidade de dados.

### 5. Acesse o Airflow

Abra o navegador e acesse: http://localhost:8080

- Usu√°rio e senha padr√£o geralmente s√£o `airflow` / `airflow` (verifique no seu docker-compose.yml)
- Execute a DAG desejada para rodar o pipeline e os testes DBT.