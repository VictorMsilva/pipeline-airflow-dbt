# pipeline-airflow-dbt

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-vicmo-blue?logo=linkedin)](https://www.linkedin.com/in/seu-perfil)
[![GitHub](https://img.shields.io/badge/GitHub-vicmo-black?logo=github)](https://github.com/seu-usuario)

## ğŸ—ï¸ VisÃ£o Geral

Pipeline de ingestÃ£o e transformaÃ§Ã£o de dados pÃºblicos utilizando Airflow e DBT. O objetivo Ã© automatizar a coleta, limpeza e disponibilizaÃ§Ã£o de dados abertos de fontes como IBGE, MinistÃ©rio da SaÃºde e Brasil.io, facilitando anÃ¡lises e relatÃ³rios reprodutÃ­veis.

## ğŸ“Œ Resumo do Projeto

Criar um pipeline de dados completo e modular para ingestÃ£o, transformaÃ§Ã£o e disponibilizaÃ§Ã£o de dados abertos, utilizando ferramentas modernas da stack de engenharia de dados.

## ğŸ©º Contexto e Problema

Ã“rgÃ£os pÃºblicos como o IBGE, MinistÃ©rio da SaÃºde e plataformas como http://Brasil.io  oferecem grandes volumes de dados Ãºteis, mas disponibilizam essas informaÃ§Ãµes de forma despadronizada: CSVs separados por ano, APIs com formatos inconsistentes e campos mal documentados.

OrganizaÃ§Ãµes ou analistas que desejam usar esses dados para monitorar tendÃªncias (ex: COVID, preÃ§os, populaÃ§Ã£o) enfrentam desafios como:

* IngestÃ£o manual ou pouco confiÃ¡vel
* Falta de automaÃ§Ã£o e versionamento
* TransformaÃ§Ãµes repetitivas e sem rastreabilidade
* Baixa reprodutibilidade de relatÃ³rios e anÃ¡lises

Este projeto resolve esse problema criando um pipeline automatizado, rastreÃ¡vel e testÃ¡vel para transformar os dados brutos em tabelas limpas e analÃ­ticas, prontas para uso em BI.

## ğŸ› ï¸ Tecnologias Utilizadas

- [Apache Airflow](https://airflow.apache.org/)
- [DBT (Data Build Tool)](https://www.getdbt.com/)
- Python 3.x
- Docker (opcional para orquestraÃ§Ã£o)
- Outras dependÃªncias listadas em `requirements.txt`

## ğŸ“ Estrutura do Projeto

```
pipeline-airflow-dbt/
â”œâ”€â”€ dags/                # DAGs do Airflow
â”œâ”€â”€ dbt/                 # Projeto DBT
â”œâ”€â”€ data/                # Dados brutos e processados
â”œâ”€â”€ scripts/             # Scripts auxiliares
â”œâ”€â”€ requirements.txt     # DependÃªncias Python
â”œâ”€â”€ docker-compose.yml   # (Opcional) OrquestraÃ§Ã£o com Docker
â””â”€â”€ README.md
```

## ğŸš€ Como Executar

1. **Clone o repositÃ³rio**
   ```bash
   git clone https://github.com/seu-usuario/pipeline-airflow-dbt.git
   cd pipeline-airflow-dbt
   ```

2. **(Opcional, mas recomendado) Crie e ative um ambiente virtual**
   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   ```

3. **Instale as dependÃªncias**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configure variÃ¡veis de ambiente**  
   (Exemplo: crie um arquivo `.env` com as credenciais necessÃ¡rias)

5. **Inicie o Airflow**
   ```bash
   airflow db init
   airflow webserver --port 8080
   airflow scheduler
   ```

6. **Execute o pipeline**
   - Acesse o Airflow em [localhost:8080](http://localhost:8080)
   - Ative e execute a DAG desejada

7. **Transforme os dados com DBT**
   ```bash
   cd dbt
   dbt run
   dbt test
   ```

## ğŸ’¡ Exemplo de Uso

ApÃ³s executar o pipeline, as tabelas limpas estarÃ£o disponÃ­veis para anÃ¡lise em ferramentas de BI ou notebooks Python.

## ğŸ¤ ContribuiÃ§Ã£o

ContribuiÃ§Ãµes sÃ£o bem-vindas!  
Abra uma issue ou envie um pull request.

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

---
Desenvolvido por [Seu Nome](https://www.linkedin.com/in/seu-perfil)


## Estrutura de Pastas Sugeridas.

## ğŸ“ Estrutura Completa do Projeto

```
pipeline-airflow-dbt/
â”œâ”€â”€ dags/                        # DAGs do Airflow (orquestraÃ§Ã£o)
â”‚   â””â”€â”€ example_dag.py
â”œâ”€â”€ dbt/                         # Projeto DBT (transformaÃ§Ãµes analÃ­ticas)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ example_model.sql
â”‚   â”œâ”€â”€ seeds/
â”‚   â”‚   â””â”€â”€ example_seed.csv
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ profiles.yml
â”œâ”€â”€ data/                        # Dados em diferentes estÃ¡gios
â”‚   â”œâ”€â”€ raw/                     # Dados brutos
â”‚   â”œâ”€â”€ processed/               # Dados processados/intermediÃ¡rios
â”‚   â””â”€â”€ analytics/               # Dados prontos para BI
â”œâ”€â”€ src/                         # CÃ³digo-fonte principal do projeto
â”‚   â”œâ”€â”€ ingestion/               # IngestÃ£o de dados
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ ibge.py
â”‚   â”‚   â”œâ”€â”€ ministerio_saude.py
â”‚   â”‚   â””â”€â”€ brasilio.py
â”‚   â”œâ”€â”€ processing/              # Limpeza e prÃ©-processamento
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ cleaning.py
â”‚   â”‚   â””â”€â”€ normalization.py
â”‚   â”œâ”€â”€ transformation/          # TransformaÃ§Ãµes fora do DBT
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ enrich.py
â”‚   â”‚   â””â”€â”€ join.py
â”‚   â”œâ”€â”€ validation/              # ValidaÃ§Ã£o e checagem de qualidade
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ schema_check.py
â”‚   â”‚   â””â”€â”€ data_quality.py
â”‚   â”œâ”€â”€ utils/                   # FunÃ§Ãµes utilitÃ¡rias
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ scripts/                     # Scripts auxiliares e pipelines
â”‚   â”œâ”€â”€ run_pipeline.py
â”‚   â””â”€â”€ setup_env.py
â”œâ”€â”€ tests/                       # Testes unitÃ¡rios e de integraÃ§Ã£o
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â””â”€â”€ test_ibge.py
â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â””â”€â”€ test_cleaning.py
â”‚   â”œâ”€â”€ transformation/
â”‚   â”‚   â””â”€â”€ test_enrich.py
â”‚   â”œâ”€â”€ validation/
â”‚   â”‚   â””â”€â”€ test_schema_check.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ test_logger.py
â”œâ”€â”€ notebooks/                   # Notebooks para exploraÃ§Ã£o e prototipagem
â”‚   â””â”€â”€ exploracao_dados.ipynb
â”œâ”€â”€ configs/                     # ConfiguraÃ§Ãµes e variÃ¡veis de ambiente
â”‚   â”œâ”€â”€ config.yaml
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ requirements.txt             # DependÃªncias Python
â”œâ”€â”€ docker-compose.yml           # OrquestraÃ§Ã£o com Docker
â””â”€â”€ README.md
```

## ğŸ› ï¸ Como preparar o ambiente manualmente

Para criar a estrutura de pastas e arquivos recomendados e instalar as dependÃªncias, siga os passos abaixo:

1. **Crie as pastas do projeto:**
   ```bash
   mkdir -p dags dbt/models dbt/seeds data/raw data/processed data/analytics \
   src/ingestion src/processing src/transformation src/validation src/utils \
   scripts tests/ingestion tests/processing tests/transformation tests/validation tests/utils \
   notebooks configs
   ```

2. **Crie os arquivos iniciais:**
   ```bash
   touch dbt/dbt_project.yml dbt/profiles.yml \
   src/ingestion/__init__.py src/ingestion/ibge.py src/ingestion/ministerio_saude.py src/ingestion/brasilio.py \
   src/processing/__init__.py src/processing/cleaning.py src/processing/normalization.py \
   src/transformation/__init__.py src/transformation/enrich.py src/transformation/join.py \
   src/validation/__init__.py src/validation/schema_check.py src/validation/data_quality.py \
   src/utils/__init__.py src/utils/logger.py src/utils/config.py src/__init__.py \
   scripts/run_pipeline.py scripts/setup_env.py \
   tests/ingestion/test_ibge.py tests/processing/test_cleaning.py tests/transformation/test_enrich.py \
   tests/validation/test_schema_check.py tests/utils/test_logger.py \
   notebooks/exploracao_dados.ipynb configs/config.yaml configs/.env.example \
   README.md requirements.txt docker-compose.yml dags/example_dag.py dbt/models/example_model.sql dbt/seeds/example_seed.csv
   ```

3. **Instale as dependÃªncias necessÃ¡rias usando [uv](https://github.com/astral-sh/uv):**
   - Certifique-se de estar com o ambiente virtual ativado.
   - Instale as dependÃªncias listadas em `requirements.txt`:
     ```bash
     uv pip install -r requirements.txt
     ```
   - Se ainda nÃ£o existir um `requirements.txt`, adicione as principais dependÃªncias do projeto:
     ```bash
     uv pip install apache-airflow dbt-core dbt-postgres pandas python-dotenv requests
     uv pip freeze > requirements.txt
     ```

Esses comandos criam toda a estrutura de diretÃ³rios e arquivos vazios recomendados para o projeto, alÃ©m de instalar as dependÃªncias essenciais com o `uv`, que Ã© mais rÃ¡pido que o pip.

