# ğŸ—ï¸ Pipeline de GeraÃ§Ã£o e TransformaÃ§Ã£o de Dados SintÃ©ticos com Docker, Airflow + DBT

## ğŸ“Œ Resumo do Projeto
Pipeline automatizado e modular para gerar, transformar e disponibilizar dados sintÃ©ticos de uma empresa fictÃ­cia de materiais de limpeza, utilizando containers Docker para facilitar a execuÃ§Ã£o e reprodutibilidade do ambiente.

---

## ğŸ©º Contexto e Problema
Empresas de varejo e atacado precisam analisar dados de vendas, clientes e estoque, mas nem sempre possuem bases de dados estruturadas para testes. A construÃ§Ã£o manual desses dados pode ser trabalhosa e pouco escalÃ¡vel.

Este projeto resolve o problema criando um pipeline baseado em Docker, que gera, transforma e disponibiliza tabelas analÃ­ticas para simular um ambiente de vendas e logÃ­stica.

---

## ğŸ¯ Objetivos
- Gerar dados sintÃ©ticos de vendas, clientes e estoque
- Containerizar o ambiente para facilitar deploy e reproduÃ§Ã£o
- Automatizar a transformaÃ§Ã£o e validaÃ§Ã£o dos dados
- Construir um pipeline escalÃ¡vel com Airflow e DBT
- Simular relatÃ³rios de BI para anÃ¡lise de desempenho

---

## ğŸ§° Stack TecnolÃ³gica
- **Containers:** Docker + Docker Compose
- **OrquestraÃ§Ã£o:** Apache Airflow
- **TransformaÃ§Ã£o:** DBT (Data Build Tool)
- **Armazenamento:** PostgreSQL (em container)
- **GeraÃ§Ã£o de Dados:** Faker, pandas
- **VisualizaÃ§Ã£o (opcional):** Metabase, Power BI ou Streamlit

---

## âš™ï¸ Estrutura do Projeto

```plaintext
pipeline-airflow-dbt/
â”œâ”€â”€ dags/                        # DAGs do Airflow (orquestraÃ§Ã£o)
â”‚   â””â”€â”€ example_dag.py
â”œâ”€â”€ dbt/                         # Projeto DBT (transformaÃ§Ãµes analÃ­ticas)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ example_model.sql
â”‚   â”œâ”€â”€ seeds/
â”‚   â”‚   â””â”€â”€ example_seed.csv
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â””â”€â”€ profiles.yml
â”œâ”€â”€ data/                        # Dados em diferentes estÃ¡gios
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ analytics/
â”œâ”€â”€ src/                         # CÃ³digo-fonte principal
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â””â”€â”€ generate_data.py
â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â””â”€â”€ cleaning.py
â”‚   â”œâ”€â”€ transformation/
â”‚   â”‚   â””â”€â”€ enrich.py
â”‚   â”œâ”€â”€ validation/
â”‚   â”‚   â””â”€â”€ schema_check.py
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ logger.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ scripts/                     # Scripts auxiliares
â”‚   â”œâ”€â”€ run_pipeline.py
â”‚   â””â”€â”€ setup_env.py
â”œâ”€â”€ tests/                       # Testes unitÃ¡rios e integraÃ§Ã£o
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â””â”€â”€ test_generate_data.py
â”‚   â”œâ”€â”€ processing/
â”‚   â”‚   â””â”€â”€ test_cleaning.py
â”‚   â”œâ”€â”€ transformation/
â”‚   â”‚   â””â”€â”€ test_enrich.py
â”‚   â”œâ”€â”€ validation/
â”‚   â”‚   â””â”€â”€ test_schema_check.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ test_logger.py
â”œâ”€â”€ notebooks/                   # Notebooks para exploraÃ§Ã£o
â”‚   â””â”€â”€ exploracao_dados.ipynb
â”œâ”€â”€ configs/                     # ConfiguraÃ§Ãµes e variÃ¡veis
â”‚   â”œâ”€â”€ config.yaml
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## ğŸ“¦ Modelagem Dimensional â€“ Projeto Vendas

### ğŸ§  Processo de NegÃ³cio
Venda de produtos por clientes, registrados em pedidos e entregues em endereÃ§os, com pagamentos e avaliaÃ§Ãµes.

### ğŸ“ GrÃ£o da Fato Principal
Cada linha representa um item vendido em um pedido.

### ğŸ“Š Tabela Fato: `fato_vendas`
- id_item_pedido
- id_pedido
- id_cliente
- id_produto
- id_vendedor
- id_data_pedido
- quantidade
- preco_unitario
- valor_total

### ğŸ§© Tabelas de DimensÃ£o

**dim_cliente**
- id_cliente
- nome
- tipo_cliente
- telefone
- email
- cidade
- estado
- data_cadastro

**dim_produto**
- id_produto
- nome_produto
- descricao
- preco_unitario
- nome_categoria
- nome_fornecedor
- cidade_fornecedor
- estado_fornecedor

**dim_vendedor**
- id_vendedor
- nome
- telefone
- email
- nome_departamento
- data_contratacao

**dim_data**
- id_data
- data
- dia
- mÃªs
- ano
- trimestre
- dia_da_semana
- feriado

---

## â­ Star Schema

RepresentaÃ§Ã£o visual da modelagem dimensional proposta:

![Esquema Estrela â€“ Modelagem Dimensional](docs/Star_schema_vendas.png)

---

## ğŸ—ï¸ Camadas do Projeto

### 1. Camada RAW / Staging (`models/staging/`)
Tratamento dos dados brutos:
- Limpeza de inconsistÃªncias
- PadronizaÃ§Ã£o de formatos
- EliminaÃ§Ã£o de nulos e duplicatas
- ConversÃµes de tipos (ex: datas, numÃ©ricos)

**Modelos incluÃ­dos:**
- `stg_clientes.sql`, `stg_enderecos.sql`, `stg_produtos.sql`
- `stg_vendedores.sql`, `stg_departamentos.sql`, `stg_pedidos.sql`
- `stg_itens_pedido.sql`, `stg_pagamentos.sql`, `stg_entregas.sql`
- `stg_categorias.sql`, `stg_fornecedores.sql`, `stg_datas.sql`, `stg_feriados.sql`

### 2. Camada de Modelagem (`models/marts/`)
Modelos incrementais que inserem dados diretamente nas tabelas fÃ­sicas criadas no banco. Cada dimensÃ£o Ã© populada e atualizada de forma incremental.

**Modelos incluÃ­dos:**
- `dim_cliente.sql`
- `dim_produto.sql`
- `dim_vendedor.sql`
- `dim_pagamento.sql`
- `dim_entrega.sql`
- `dim_data.sql`
- `fato_vendas.sql`

## ğŸ§ª Testes Automatizados

Cada dimensÃ£o e fato tem testes definidos via arquivos `.yml`, como:
- `not_null` â€” campos obrigatÃ³rios
- `unique` â€” chaves primÃ¡rias
- `accepted_values` â€” domÃ­nios vÃ¡lidos
- `relationships` â€” integridade referencial

## ğŸ“š DocumentaÃ§Ã£o DBT e Data Lineage

O projeto utiliza o [DBT Docs](https://docs.getdbt.com/docs/building-a-dbt-project/documentation) para documentar todos os modelos, fontes, testes e dependÃªncias do pipeline de dados.

- Para visualizar a documentaÃ§Ã£o interativa e o lineage graph, rode:

  dbt docs generate
  dbt docs serve --port 8080

## ğŸš¦ OrquestraÃ§Ã£o com Apache Airflow

O projeto utiliza o [Apache Airflow](https://airflow.apache.org/) para orquestrar e automatizar o pipeline de dados, garantindo que as etapas de ingestÃ£o, transformaÃ§Ã£o (DBT) e validaÃ§Ã£o sejam executadas de forma controlada e agendada.

- As DAGs (pipelines) ficam na pasta `dags/`.
- O Airflow Ã© executado em container Docker, acessÃ­vel em [http://localhost:8080](http://localhost:8080).

## âœ… Boas PrÃ¡ticas de Engenharia de Dados

- **SeparaÃ§Ã£o de camadas:** Dados brutos, staging e modelos analÃ­ticos organizados em pastas distintas.
- **Versionamento e reprodutibilidade:** Todo o pipeline Ã© versionado no Git e executÃ¡vel via Docker.
- **Gerenciamento seguro de variÃ¡veis:** Credenciais e parÃ¢metros sensÃ­veis em arquivos de ambiente, nunca no cÃ³digo.
- **Testes automatizados:** DBT garante integridade dos dados com testes de unicidade, nulidade e relacionamentos.
- **OrquestraÃ§Ã£o modular:** Airflow agenda, monitora e facilita a extensÃ£o dos pipelines.
- **DocumentaÃ§Ã£o e lineage:** DBT Docs gera documentaÃ§Ã£o automÃ¡tica e visualizaÃ§Ã£o do fluxo de dados.

## ğŸ“ˆ Entrega de Resultados

- **Tabelas analÃ­ticas validadas:** Prontas para BI e anÃ¡lises.
- **Pipeline automatizado:** Da ingestÃ£o Ã  disponibilizaÃ§Ã£o dos dados, tudo monitorado pelo Airflow.
- **Ambiente reproduzÃ­vel:** Qualquer pessoa pode clonar e rodar o projeto do zero.
- **Pronto para integraÃ§Ã£o:** Dados finais conectÃ¡veis a ferramentas como Metabase ou Power BI.